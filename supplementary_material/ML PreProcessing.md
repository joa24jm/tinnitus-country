## Machine Learning preprocessing
For the machine learning task, a further preprocessing was required. Gradient boosting machines can only handle numerical data with no missing values. We therefore dropped rows that contained missing values, which affected about 24 \% of the data. We then needed to convert categorical features into numbers. We used Pandas' pd.get\_dummies() method to convert the countries and seasons into several columns. The column name is then the category. A \texttt{1} indicates that this category applies, i.e., \texttt{autumn = 1}, which, in turn, means that the other seasons must be zero. In order not to increase the number of columns unnecessarily, we used the \texttt{drop\_first = True} keyword argument. This means, we get k-1 dummies out of k categorical levels by removing the first level.
The last step considered the imbalanced distribution of the target variable \textit{tinnitus occurrence}. About 79 \% of the assessments state \textit{yes}. Any naive machine learning classifier would therefore simply always predict \textit{yes}, regardless of the input of features and would still get 79 \% accuracy on average. Under a naive, dummy classifier we subsume an algorithm that always predicts the majority class. Using the F1 accuracy score, the performance can be measured better, but the classifier would still be overfitted on positive examples. We therefore bootstrapped negative examples with replacement until we had a balanced dataset. The final dataset had 118,054 samples with 22 features each. 

## Gradient Boosting Machines for classification and regression
Why did we choose the Gradient Boosting Machine? It is a tree-based Machine Learning algorithm and related to Random Forests. Unlike the Random Forest, the Gradient Boosting Machine iteratively learns from the weak points of previously trained trees and therefore tends to show better performance for tabular data. Machine Learning contests on the Kaggle platform have recently shown that this algorithm is superior to most state-of-the-art Deep Learning methods when it comes to tabular data, such as house pricing prediction problems. Both, Random Forests and Gradient Boosting Machines use several trees to predict the outcome. However, one of the main differences between those two algorithms is the \textit{time aspect}. That is, the Gradient Boosting algorithm learns from previous miss-classified samples by putting more weight on those. Furthermore, it does not easily tend to overfitting like decision trees do.\\
We set the 20 features (10 countries, 4 seasons, sex, age, mood, arousal, stress, concentration level) and the targets (\texttt{momentary tinnitus, tinnitus loudness}. The whole dataset was divided into three sets: Training, development, and testing.
Training plus development got 70 \% of the whole data, testing the remaining 30 \%. To avoid a selection bias within the classification problem, we stratified on $y$. Setting a \texttt{random\_state} (also known as seed) ensured that the results are reproducible. For the tuning of the hyperparameters, we used a gridsearch approach. Within that, we varied the \texttt{learning\_rate}, the \texttt{max\_depth} of each tree, the sizes of the \texttt{subsample}s, the minimum number of samples per leaf, and the fraction of randomly chosen features per tree. 1,280 combinations of the hyperparameters have been evaluated systematically, the final chosen setup can be found in the via the supplementary material on GitHub. Each combination was cross-validated within the training set using a 5-fold split. This means that the 70 \% of the training data was further divided into 5 folds. Four of each were used for training and one for validation.\\